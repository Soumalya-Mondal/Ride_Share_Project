# ğŸš– Taxi Lookup & Ride Share Database Setup

This project sets up PostgreSQL tables for **Taxi Lookup Zones** and **Ride Share** data, and loads large-scale datasets efficiently using GPU-accelerated data processing.

---

## ğŸ“˜ Overview

The project automates:
- Creation of two PostgreSQL tables:
  - **taxi_zone_lookup**
  - **ride_share_data**
- Efficient data loading into PostgreSQL using **cuDF** (GPU DataFrames) and **psycopg2**.

The **Ride Share** dataset is large (~12 GB, `.parquet` format), requiring GPU acceleration for faster loading and transformation.

---

## âš™ï¸ Technologies Used

| Technology | Purpose |
|-------------|----------|
| **PostgreSQL** | Database to store Taxi Lookup and Ride Share data |
| **Python** | Automation and data handling |
| **cuDF** | GPU-based DataFrame library for handling large `.parquet` files |
| **psycopg2** | PostgreSQL database connection and operations |
| **python-dotenv** | Load database credentials securely from `.env` file |
| **uv** | Fast Python package manager and virtual environment manager |

---

## ğŸ§© Project Structure



```
.
â””â”€â”€ support/
    â”œâ”€â”€ taxi_zone_lookup_table_create.py  # Creates Taxi Lookup Zone table
    â””â”€â”€ rideshare_data_table_create.py    # Creates Ride Share table
    â””â”€â”€ taxi_zone_lookup_data_entry.py    # Inserts data into Taxi Lookup Zone
    â””â”€â”€ rideshare_data_data_entry.py      # Inserts data into Ride Share
â”œâ”€â”€ main.py                               # Main Script
â”œâ”€â”€ .env                                  # Stores DB credentials 
â”œâ”€â”€ README.md                             # Project documentation
â”œâ”€â”€ .python-version                       # Specifies the Python version used in development
â”œâ”€â”€ pyproject.toml                        # Project metadata and dependency definitions for uv
â”œâ”€â”€ uv.lock                               # Lock file with exact dependency versions
â””â”€â”€ Data_Archive/
    â”œâ”€â”€ taxi_zone_lookup.csv              # Lookup data (example)
    â””â”€â”€ rideshare_data.parquet            # Large Ride Share dataset (~12GB)
```

---

## ğŸš€ How to Run

### ğŸ§± Step 1: Install Dependencies
```bash
uv sync
```

### 2ï¸âƒ£ Step 2: Execute Script
```bash
uv run main.py
```

---

## âš¡ Performance Note

The **Ride Share** `.parquet` file (~12 GB) is processed using **cuDF** for GPU-accelerated reading and transformation before inserting into PostgreSQL.  
This drastically reduces load time compared to CPU-only approaches.

---

## ğŸ§  Requirements

- Python 3.9+
- PostgreSQL 13+
- NVIDIA GPU with CUDA support
- uv package manager
- Python Libraries:
  ```bash
  uv add cudf-cu12 psycopg2-binary python-dotenv
  ```

---

## ğŸ—‚ï¸ Database Configuration

Store your PostgreSQL credentials in a `.env` file located at the project root:

```env
DB_HOST=localhost
DB_PORT=5432
DB_NAME=your_db
DB_USERNAME=username
DB_PASSWORD=yourpassword
```

Each script automatically loads these credentials using **python-dotenv**.

Example connection snippet used in all scripts:

---

## ğŸŒ Data Source

The Ride Share dataset is publicly available on Kaggle:  
ğŸ”— [NYC Rideshare Raw Data â€“ Kaggle](https://www.kaggle.com/datasets/aaronweymouth/nyc-rideshare-raw-data)

---

## ğŸ“„ License

This project is licensed under the **MIT License**.